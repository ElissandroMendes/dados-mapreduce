{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:28:45.128452Z",
     "start_time": "2020-09-24T18:28:45.108497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aff22e3cc04e8cbc91d5a5ea444f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1601104146332_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-81-14.ec2.internal:20888/proxy/application_1601104146332_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-88-162.ec2.internal:8042/node/containerlogs/container_1601104146332_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ajustando locale da JVM para en-US, estava misturando os locale e ficando en-BR...\n",
    "# Fonte: https://stackoverflow.com/questions/55246080/pyspark-stopwordsremover-parameter-locale-given-invalid-value\n",
    "locale = spark.sparkContext._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce967eb6cad94a758e657965e0f751b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:34:38.397057Z",
     "start_time": "2020-09-24T18:34:38.381605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bed26663954afc9c80af877c33d26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "def load_file(filePath, type, delimiter = None):\n",
    "    if type == 'text':\n",
    "        df = spark.read.load(filePath, format='text')\n",
    "    else:\n",
    "        df = spark.read.csv(filePath, inferSchema=True, header=True, sep=delimiter)\n",
    "    return df\n",
    "\n",
    "def concat_all_reviews_v1(reviews):\n",
    "    \"\"\"\n",
    "        Concatena todas as reviews (linhas no DF) em um RDD no formato (1, TEXTO).\n",
    "        Retorna um RDD\n",
    "    \"\"\"\n",
    "    rdd = reviews.rdd.map(lambda r: (1, r[0])).reduceByKey(lambda a, b: a + ' ' + b)\n",
    "    return rdd\n",
    "\n",
    "def concat_all_reviews_v2(reviews):\n",
    "    \"\"\"\n",
    "        Concatena todas as reviews (linhas no DF) em um texto único.\n",
    "        Retorna uma string\n",
    "    \"\"\"\n",
    "    rdd = reviews.rdd.flatMap(lambda v: v[0].split()).reduce(lambda a, b: a + ' ' + b)\n",
    "    return rdd\n",
    "\n",
    "def tokenize_reviews(reviews):\n",
    "    tokenization = Tokenizer(inputCol='review', outputCol='review_words')\n",
    "    return tokenization.transform(reviews)\n",
    "\n",
    "def remove_stop_words_reviews(reviews):\n",
    "    stopword_removal = StopWordsRemover(inputCol='review_words',outputCol='review_words_refined')\n",
    "    return stopword_removal.transform(reviews)\n",
    "\n",
    "def counter_words_rdd_flatMap(df, index):\n",
    "    \"\"\"\n",
    "        Filtra e conta palavras usando RDD flatMap\n",
    "    \"\"\"\n",
    "    filtered = refined_reviews \\\n",
    "                .rdd \\\n",
    "                .flatMap(lambda v: [(w, 1) for w in v[index] if w != '' and len(w) > 3]) \\\n",
    "                .reduceByKey(lambda c1, c2: c1 + c2)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T22:08:07.155052Z",
     "start_time": "2020-09-24T22:07:54.880043Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee68e5825444ad69b836b445c2993ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:5.099673669999902 sec\n",
      "---------------------------\n",
      "Round 1\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.7620989409988397 sec\n",
      "---------------------------\n",
      "Round 2\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6737163289981254 sec\n",
      "---------------------------\n",
      "Round 3\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.667520966999291 sec\n",
      "---------------------------\n",
      "Round 4\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6980949889984913 sec\n",
      "---------------------------\n",
      "Round 5\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6487892279983498 sec\n",
      "---------------------------\n",
      "Round 6\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6815106440008094 sec\n",
      "---------------------------\n",
      "Round 7\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6588389219978126 sec\n",
      "---------------------------\n",
      "Round 8\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.5343848390002677 sec\n",
      "---------------------------\n",
      "Round 9\n",
      "Loading file: mobile_test.tsv\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.5844333530003496 sec\n",
      "---------------------------\n",
      "Writing exec logs...\n",
      "[('everything', 3), ('careful', 2), ('listening', 1), ('wonderful', 1), ('park,', 1), ('*buzzer*,', 1), ('item', 1), ('hmmm...maybe', 1), ('needs', 2), ('african', 1)]"
     ]
    }
   ],
   "source": [
    "\n",
    "#hdfs_folder = 'hdfs://ip-172-31-91-35.ec2.internal:8020/user/livy/'\n",
    "\n",
    "now = datetime.now()\n",
    "execs_per_file = 10\n",
    "\n",
    "files = [\n",
    "    \"mobile_test.tsv\",\n",
    "   # \"mobile_test2x.tsv\",\n",
    "   # \"mobile_test3x.tsv\",\n",
    "   # \"mobile_test4x.tsv\",\n",
    "   # \"mobile_test8x.tsv\",\n",
    "   # \"mobile_test10k.tsv\",\n",
    "  #  \"mobile_test100k.tsv\",\n",
    "  #  \"mobile_test200k.tsv\"\n",
    "]\n",
    "\n",
    "for f in files:    \n",
    "    lines = []\n",
    "    for r in np.arange(0, execs_per_file):\n",
    "        print(\"Round \" + str(r))\n",
    "        \n",
    "        print(f\"Loading file: {f}\")\n",
    "        content = load_file(f\"{f}\", type='tsv', delimiter='\\t')\n",
    "        \n",
    "        start = timer()\n",
    "        print(\"Removing empty reviews...\")\n",
    "        content = content.select('review_body').filter(content.review_body != '')\n",
    "        \n",
    "        print(\"Concating content...\")\n",
    "        all_reviews_rdd = concat_all_reviews_v1(content)\n",
    "        \n",
    "        print(\"Creating DF to Mlib...\")\n",
    "        # Cria DataFrame para uso com a Mlib\n",
    "        all_reviews_df = spark.createDataFrame(data=all_reviews_rdd, schema=['id', 'review'])\n",
    "        \n",
    "        print(\"Tokenization...\")\n",
    "        all_reviews_df_tokenized = tokenize_reviews(all_reviews_df)\n",
    "        \n",
    "        print(\"Removing StopWords...\")\n",
    "        refined_reviews = remove_stop_words_reviews(all_reviews_df_tokenized)\n",
    "        \n",
    "        print(\"Filter and counter Words...\")\n",
    "        filtered = counter_words_rdd_flatMap(refined_reviews, 3)\n",
    "        \n",
    "        end = timer()\n",
    "        exec_time = end - start\n",
    "        l = (f, str(r), \"{:.4f}\".format(exec_time))\n",
    "        lines.append(l)\n",
    "        print(f\"Execution time:{exec_time} sec\")\n",
    "        print(\"---------------------------\")\n",
    "        \n",
    "    print(\"Writing exec logs...\")\n",
    "    logs_execs = spark.createDataFrame(data=lines, schema=['file', 'round', 'time'])\n",
    "    \n",
    "    log_file = f\"s3://aws-emr-resources-188023916529-us-east-1/notebooks/e-8IEIV3G0OF4JFD9X5P7TBISY6/execs_new6\"\n",
    "    logs_execs.repartition(1).write.csv(path=log_file, mode=\"append\", header=\"true\")\n",
    "    \n",
    "    # logs_execs.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save(log_file)\n",
    "\n",
    "filtered.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
