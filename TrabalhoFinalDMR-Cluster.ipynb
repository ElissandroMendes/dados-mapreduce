{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:27:55.799868Z",
     "start_time": "2020-09-24T18:27:55.794071Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "fromm datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda3/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda3/bin/python\"\n",
    "\n",
    "os.environ['HADOOP_LOG_DIR'] = '/Users/elissandro/Projetos/Especializacao/MDMR/logs'\n",
    "os.environ['HADOOP_CONF_DIR'] = '/Users/elissandro/Projetos/Especializacao/MDMR/conf/spark'\n",
    "os.environ['HADOOP_USER_NAME'] = 'elissandro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:28:40.284249Z",
     "start_time": "2020-09-24T18:27:57.414983Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "    \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Trabalho Final - DMR') \\\n",
    "    .config('spark.driver.host', '192.168.1.37') \\\n",
    "    .master('yarn') \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:28:45.128452Z",
     "start_time": "2020-09-24T18:28:45.108497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ajustando locale da JVM para en-US, estava misturando os locale e ficando en-BR...\n",
    "# Fonte: https://stackoverflow.com/questions/55246080/pyspark-stopwordsremover-parameter-locale-given-invalid-value\n",
    "locale = spark.sparkContext._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:34:38.397057Z",
     "start_time": "2020-09-24T18:34:38.381605Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_file(filePath, type, delimiter = None):\n",
    "    if type == 'text':\n",
    "        df = spark.read.load(filePath, format='text')\n",
    "    else:\n",
    "        df = spark.read.csv(filePath, inferSchema=True, header=True, sep=delimiter)\n",
    "    return df\n",
    "\n",
    "def concat_all_reviews_v1(reviews):\n",
    "    \"\"\"\n",
    "        Concatena todas as reviews (linhas no DF) em um RDD no formato (1, TEXTO).\n",
    "        Retorna um RDD\n",
    "    \"\"\"\n",
    "    rdd = reviews.rdd.map(lambda r: (1, r[0])).reduceByKey(lambda a, b: a + ' ' + b)\n",
    "    return rdd\n",
    "\n",
    "def concat_all_reviews_v2(reviews):\n",
    "    \"\"\"\n",
    "        Concatena todas as reviews (linhas no DF) em um texto Ãºnico.\n",
    "        Retorna uma string\n",
    "    \"\"\"\n",
    "    rdd = reviews.rdd.flatMap(lambda v: v[0].split()).reduce(lambda a, b: a + ' ' + b)\n",
    "    return rdd\n",
    "\n",
    "def tokenize_reviews(reviews):\n",
    "    tokenization = Tokenizer(inputCol='review', outputCol='review_words')\n",
    "    return tokenization.transform(reviews)\n",
    "\n",
    "def remove_stop_words_reviews(reviews):\n",
    "    stopword_removal = StopWordsRemover(inputCol='review_words',outputCol='review_words_refined')\n",
    "    return stopword_removal.transform(reviews)\n",
    "\n",
    "def counter_words_rdd_flatMap(df, index):\n",
    "    \"\"\"\n",
    "        Filtra e conta palavras usando RDD flatMap\n",
    "    \"\"\"\n",
    "    filtered = refined_reviews \\\n",
    "                .rdd \\\n",
    "                .flatMap(lambda v: [(w, 1) for w in v[index] if w != '' and len(w) > 3]) \\\n",
    "                .reduceByKey(lambda c1, c2: c1 + c2)\n",
    "    return filtered\n",
    "\n",
    "def counter_words_sql_expr(df, colunmName):\n",
    "    \"\"\"\n",
    "        Filtra e conta palavras usando SparkSQL\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import expr\n",
    "    \n",
    "    filter_expr = f\"filter({colunmName}, x -> x != '' and char_length(x) > 3)\"\n",
    "    filtered = df.withColumn(colunmName, expr(filter_expr)).rdd.flatMap(lambda v: v[0])\n",
    "\n",
    "    return filtered.groupBy(colunmName).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T22:08:07.155052Z",
     "start_time": "2020-09-24T22:07:54.880043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: mobile_test.tsv\n",
      "Round 0\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:1.0862637780010118 sec\n",
      "---------------------------\n",
      "Round 1\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.7496607160010171 sec\n",
      "---------------------------\n",
      "Round 2\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:1.0919096670004365 sec\n",
      "---------------------------\n",
      "Round 3\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:1.7772060750012315 sec\n",
      "---------------------------\n",
      "Round 4\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:1.0174028420005925 sec\n",
      "---------------------------\n",
      "Round 5\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6584527699997125 sec\n",
      "---------------------------\n",
      "Round 6\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6932758599996305 sec\n",
      "---------------------------\n",
      "Round 7\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6192303160005395 sec\n",
      "---------------------------\n",
      "Round 8\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:1.37651605100109 sec\n",
      "---------------------------\n",
      "Round 9\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.7355444849999913 sec\n",
      "---------------------------\n",
      "Writing exec logs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('advertised.', 4),\n",
       " ('everything', 3),\n",
       " ('works', 16),\n",
       " ('perfectly,', 1),\n",
       " ('happy', 4),\n",
       " ('camera.', 1),\n",
       " ('matter', 1),\n",
       " ('fact', 1),\n",
       " ('going', 2),\n",
       " ('another', 4)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_folder = 'hdfs://192.168.1.37:9000/trabalho-final'\n",
    "\n",
    "local_path = '/Users/elissandro/Projetos/Especializacao/MDMR/logs/exec_logs'\n",
    "if not os.path.exists(local_path):\n",
    "    os.makedirs(local_folder)\n",
    "    \n",
    "exec_logs_folder = f\"file://{local_path}\"\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"local_folder\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "files = [\"mobile_test.tsv\"]\n",
    "\n",
    "lines = []\n",
    "exec_info = []\n",
    "execs_per_file = 10\n",
    "\n",
    "for f in files:    \n",
    "    print(f\"Loading file: {f}\")\n",
    "    content = load_file(f\"{hdfs_folder}/{f}\", type='tsv', delimiter='\\t')\n",
    "    for r in np.arange(0, execs_per_file):\n",
    "        print(\"Round \" + str(r))\n",
    "\n",
    "        start = timer()\n",
    "        print(\"Removing empty reviews...\")\n",
    "        content = content.select('review_body').filter(content.review_body != '')\n",
    "        \n",
    "        print(\"Concating content...\")\n",
    "        all_reviews_rdd = concat_all_reviews_v1(content)\n",
    "        \n",
    "        print(\"Creating DF to Mlib...\")\n",
    "        # Cria DataFrame para uso com a Mlib\n",
    "        all_reviews_df = spark.createDataFrame(data=all_reviews_rdd, schema=['id', 'review'])\n",
    "        \n",
    "        print(\"Tokenization...\")\n",
    "        all_reviews_df_tokenized = tokenize_reviews(all_reviews_df)\n",
    "        \n",
    "        print(\"Removing StopWords...\")\n",
    "        refined_reviews = remove_stop_words_reviews(all_reviews_df_tokenized)\n",
    "        \n",
    "        print(\"Filter and counter Words...\")\n",
    "        filtered = counter_words_rdd_flatMap(refined_reviews, 3)\n",
    "        \n",
    "        end = timer()\n",
    "        exec_time = end - start\n",
    "        l = (f, str(r), \"{:.4f}\".format(exec_time))\n",
    "        lines.append(l)\n",
    "        print(f\"Execution time:{exec_time} sec\")\n",
    "        print(\"---------------------------\")\n",
    "        \n",
    "    print(\"Writing exec logs...\")\n",
    "    logs_execs = spark.createDataFrame(data=lines, schema=['file', 'round', 'time'])\n",
    "    # logs_execs.write.csv(f\"{exec_logs_folder}/exec_{f}_{now.strftime('%Y%m%d-%H%M%S')}.csv\")\n",
    "    logs_execs.toPandas() \\\n",
    "        .to_csv(f\"exec_{now.strftime('%Y%m%d-%H%M%S')}.csv\", index = False, header=True)\n",
    "#        .to_csv(f\"{exec_logs_folder}/exec_{now.strftime('%Y%m%d-%H%M%S')}.csv\", index = False, header=True)\n",
    "\n",
    "filtered.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:06:29.950781Z",
     "start_time": "2020-09-24T21:06:29.945821Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:12:03.544412Z",
     "start_time": "2020-09-24T21:12:03.538071Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "Path('local_folder').mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:14:11.291019Z",
     "start_time": "2020-09-24T21:14:11.282443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elissandro/Projetos/Especializacao/MDMR/logs/exec_logs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(local_folder)\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
