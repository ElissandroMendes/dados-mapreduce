{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:27:55.799868Z",
     "start_time": "2020-09-24T18:27:55.794071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54aafe9317854ea88d8e518772348eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1600992860161_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-81-222.ec2.internal:20888/proxy/application_1600992860161_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-87-230.ec2.internal:8042/node/containerlogs/container_1600992860161_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda3/bin/python\"\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda3/bin/python\"\n",
    "\n",
    "#os.environ['HADOOP_LOG_DIR'] = '/Users/elissandro/Projetos/Especializacao/MDMR/logs'\n",
    "#os.environ['HADOOP_CONF_DIR'] = '/Users/elissandro/Projetos/Especializacao/MDMR/conf/spark'\n",
    "#os.environ['HADOOP_USER_NAME'] = 'elissandro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:28:45.128452Z",
     "start_time": "2020-09-24T18:28:45.108497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df37bb838ac4db8b9a7a6a5e4e63042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ajustando locale da JVM para en-US, estava misturando os locale e ficando en-BR...\n",
    "# Fonte: https://stackoverflow.com/questions/55246080/pyspark-stopwordsremover-parameter-locale-given-invalid-value\n",
    "locale = spark.sparkContext._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T18:34:38.397057Z",
     "start_time": "2020-09-24T18:34:38.381605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ab30e56da14610bce87df1dae42843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "def load_file(filePath, type, delimiter = None):\n",
    "    if type == 'text':\n",
    "        df = spark.read.load(filePath, format='text')\n",
    "    else:\n",
    "        df = spark.read.csv(filePath, inferSchema=True, header=True, sep=delimiter)\n",
    "    return df\n",
    "\n",
    "def concat_all_reviews_v1(reviews):\n",
    "    \"\"\"\n",
    "        Concatena todas as reviews (linhas no DF) em um RDD no formato (1, TEXTO).\n",
    "        Retorna um RDD\n",
    "    \"\"\"\n",
    "    rdd = reviews.rdd.map(lambda r: (1, r[0])).reduceByKey(lambda a, b: a + ' ' + b)\n",
    "    return rdd\n",
    "\n",
    "def concat_all_reviews_v2(reviews):\n",
    "    \"\"\"\n",
    "        Concatena todas as reviews (linhas no DF) em um texto único.\n",
    "        Retorna uma string\n",
    "    \"\"\"\n",
    "    rdd = reviews.rdd.flatMap(lambda v: v[0].split()).reduce(lambda a, b: a + ' ' + b)\n",
    "    return rdd\n",
    "\n",
    "def tokenize_reviews(reviews):\n",
    "    tokenization = Tokenizer(inputCol='review', outputCol='review_words')\n",
    "    return tokenization.transform(reviews)\n",
    "\n",
    "def remove_stop_words_reviews(reviews):\n",
    "    stopword_removal = StopWordsRemover(inputCol='review_words',outputCol='review_words_refined')\n",
    "    return stopword_removal.transform(reviews)\n",
    "\n",
    "def counter_words_rdd_flatMap(df, index):\n",
    "    \"\"\"\n",
    "        Filtra e conta palavras usando RDD flatMap\n",
    "    \"\"\"\n",
    "    filtered = refined_reviews \\\n",
    "                .rdd \\\n",
    "                .flatMap(lambda v: [(w, 1) for w in v[index] if w != '' and len(w) > 3]) \\\n",
    "                .reduceByKey(lambda c1, c2: c1 + c2)\n",
    "    return filtered\n",
    "\n",
    "def counter_words_sql_expr(df, colunmName):\n",
    "    \"\"\"\n",
    "        Filtra e conta palavras usando SparkSQL\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import expr\n",
    "    \n",
    "    filter_expr = f\"filter({colunmName}, x -> x != '' and char_length(x) > 3)\"\n",
    "    filtered = df.withColumn(colunmName, expr(filter_expr)).rdd.flatMap(lambda v: v[0])\n",
    "\n",
    "    return filtered.groupBy(colunmName).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T22:08:07.155052Z",
     "start_time": "2020-09-24T22:07:54.880043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c41c25e1a7a4b1f8e55682e49b2bc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 0\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:3.9530566000012186 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 1\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.5652271480012132 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 2\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.5553559740001219 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 3\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.539568677000716 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 4\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.5301577939990239 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 5\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.5274707820008189 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 6\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.6882642780001333 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 7\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.8594215200009785 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 8\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.7820522920010262 sec\n",
      "---------------------------\n",
      "Loading file: hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\n",
      "Round 9\n",
      "Removing empty reviews...\n",
      "Concating content...\n",
      "Creating DF to Mlib...\n",
      "Tokenization...\n",
      "Removing StopWords...\n",
      "Filter and counter Words...\n",
      "Execution time:0.5610451760003343 sec\n",
      "---------------------------\n",
      "Writing exec logs...\n",
      "[('advertised.', 4), ('everything', 3), ('works', 16), ('perfectly,', 1), ('happy', 4), ('camera.', 1), ('matter', 1), ('fact', 1), ('going', 2), ('another', 4)]"
     ]
    }
   ],
   "source": [
    "hdfs_folder = 'hdfs://192.168.1.37:9000/trabalho-final'\n",
    "\n",
    "#local_path = '/Users/elissandro/Projetos/Especializacao/MDMR/logs/exec_logs'\n",
    "#if not os.path.exists(local_path):\n",
    "#    os.makedirs(local_folder)\n",
    "    \n",
    "#exec_logs_folder = f\"file://{local_path}\"\n",
    "\n",
    "#from pathlib import Path\n",
    "#Path(\"local_folder\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "files = [\"hdfs://ip-172-31-81-222.ec2.internal:8020/user/mobile_test_2.tsv\"]\n",
    "\n",
    "lines = []\n",
    "exec_info = []\n",
    "execs_per_file = 10\n",
    "\n",
    "for f in files:    \n",
    "    #content = load_file(f\"{hdfs_folder}/{f}\", type='tsv', delimiter='\\t')\n",
    "    for r in np.arange(0, execs_per_file):\n",
    "        print(\"Round \" + str(r))\n",
    "        \n",
    "        print(f\"Loading file: {f}\")\n",
    "        content = load_file(f, type='tsv', delimiter='\\t')\n",
    "\n",
    "        start = timer()\n",
    "        print(\"Removing empty reviews...\")\n",
    "        content = content.select('review_body').filter(content.review_body != '')\n",
    "        \n",
    "        print(\"Concating content...\")\n",
    "        all_reviews_rdd = concat_all_reviews_v1(content)\n",
    "        \n",
    "        print(\"Creating DF to Mlib...\")\n",
    "        # Cria DataFrame para uso com a Mlib\n",
    "        all_reviews_df = spark.createDataFrame(data=all_reviews_rdd, schema=['id', 'review'])\n",
    "        \n",
    "        print(\"Tokenization...\")\n",
    "        all_reviews_df_tokenized = tokenize_reviews(all_reviews_df)\n",
    "        \n",
    "        print(\"Removing StopWords...\")\n",
    "        refined_reviews = remove_stop_words_reviews(all_reviews_df_tokenized)\n",
    "        \n",
    "        print(\"Filter and counter Words...\")\n",
    "        filtered = counter_words_rdd_flatMap(refined_reviews, 3)\n",
    "        \n",
    "        end = timer()\n",
    "        exec_time = end - start\n",
    "        l = (f, str(r), \"{:.4f}\".format(exec_time))\n",
    "        lines.append(l)\n",
    "        print(f\"Execution time:{exec_time} sec\")\n",
    "        print(\"---------------------------\")\n",
    "        \n",
    "    print(\"Writing exec logs...\")\n",
    "    logs_execs = spark.createDataFrame(data=lines, schema=['file', 'round', 'time'])\n",
    "    # logs_execs.write.csv(f\"{exec_logs_folder}/exec_{f}_{now.strftime('%Y%m%d-%H%M%S')}.csv\")\n",
    "    \n",
    "    log_file = f\"s3://aws-emr-resources-188023916529-us-east-1/notebooks/e-8IEIV3G0OF4JFD9X5P7TBISY6/exec_{now.strftime('%Y%m%d-%H%M%S')}.csv\"\n",
    "    logs_execs.repartition(1).write.csv(path=log_file, mode=\"append\", header=\"true\")\n",
    "    \n",
    "    # logs_execs.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save(log_file)\n",
    "\n",
    "\n",
    "filtered.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c465eff59e7b43b7a1862c5c2da664c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from https://172.31.81.222:18888/sessions/0 with error payload: {\"msg\":\"Session '0' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:06:29.950781Z",
     "start_time": "2020-09-24T21:06:29.945821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9caa02d10e5d49aa99fd25ad21935af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a4698f23924c58933c1a6779e91841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- <!DOCTYPE html>: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "content = load_file(f, type='tsv', delimiter='\\t')\n",
    "content.printSchema()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
